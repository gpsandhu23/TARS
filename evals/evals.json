["```json\n[\n  {\n    \"question\": \"What is the core controller of LLM-powered autonomous agents?\",\n    \"ideal_answer\": \"The core controller of LLM-powered autonomous agents is a large language model (LLM).\"\n  },\n  {\n    \"question\": \"What are the key components in an LLM-powered autonomous agent system?\",\n    \"ideal_answer\": \"The key components in an LLM-powered autonomous agent system include Planning, Memory, and Tool Use.\"\n  },\n  {\n    \"question\": \"How do autonomous agents perform task decomposition?\",\n    \"ideal_answer\": \"Autonomous agents perform task decomposition by breaking down large tasks into smaller, manageable subgoals using techniques like Chain of Thought (CoT) and Tree of Thoughts, or by leveraging external classical planners with PDDL for long-horizon planning.\"\n  },\n  {\n    \"question\": \"What is the role of memory in LLM-powered autonomous agents?\",\n    \"ideal_answer\": \"The role of memory in LLM-powered autonomous agents is to acquire, store, retain, and later retrieve information. It includes short-term memory for in-context learning and long-term memory for retaining information over extended periods, often facilitated by external vector stores.\"\n  },\n  {\n    \"question\": \"How do LLM-powered autonomous agents use tools?\",\n    \"ideal_answer\": \"LLM-powered autonomous agents learn to call external APIs for extra information, execute code, and access proprietary information sources to extend their capabilities beyond what is stored in the model weights.\"\n  },\n  {\n    \"question\": \"What are some examples of proof-of-concept demos for LLM-powered autonomous agents?\",\n    \"ideal_answer\": \"Examples of proof-of-concept demos for LLM-powered autonomous agents include AutoGPT, GPT-Engineer, and BabyAGI, showcasing the potential of LLMs as general problem solvers.\"\n  },\n  {\n    \"question\": \"What is the significance of Maximum Inner Product Search (MIPS) in the context of LLM-powered autonomous agents?\",\n    \"ideal_answer\": \"The significance of Maximum Inner Product Search (MIPS) lies in its ability to optimize the retrieval speed from an external memory vector store, enabling efficient access to a large knowledge pool beyond the agent's immediate context window.\"\n  },\n  {\n    \"question\": \"How do autonomous agents improve through self-reflection?\",\n    \"ideal_answer\": \"Autonomous agents improve through self-reflection by refining past action decisions, correcting previous mistakes, and learning from trial and error. This is achieved using mechanisms like ReAct, Reflexion, and Chain of Hindsight to iteratively enhance reasoning and decision-making skills.\"\n  },\n  {\n    \"question\": \"What challenges do LLM-powered autonomous agents face?\",\n    \"ideal_answer\": \"LLM-powered autonomous agents face challenges such as finite context length, difficulties in long-term planning and task decomposition, and the reliability of natural language interfaces for communication with external components.\"\n  },\n  {\n    \"question\": \"What is the purpose of the API-Bank benchmark?\",\n    \"ideal_answer\": \"The purpose of the API-Bank benchmark is to evaluate the performance of tool-augmented LLMs in making API calls, retrieving the right API based on user requirements, and planning multiple API calls to solve complex tasks. It assesses the agent's tool use capabilities at different levels.\"\n  }\n]\n```", "```json\n[\n  {\n    \"question\": \"What is the main source of task-specific labeled data for deep learning models?\",\n    \"ideal_answer\": \"The main source of task-specific labeled data for deep learning models comes from human annotation, such as classification tasks or RLHF labeling for LLM alignment training.\"\n  },\n  {\n    \"question\": \"What are the key steps involved in collecting high-quality human data?\",\n    \"ideal_answer\": \"Key steps involved in collecting high-quality human data include task design to improve clarity and reduce complexity, selecting and training a pool of raters with matched skillset and consistency, and collecting and aggregating data using ML techniques to clean, filter, and smartly aggregate data to identify true labels.\"\n  },\n  {\n    \"question\": \"What does the phrase 'Vox populi' mean and how does it relate to data collection?\",\n    \"ideal_answer\": \"'Vox populi' means 'the voice of the people' in Latin. It relates to data collection through the concept of crowdsourcing or the wisdom of the crowd, where aggregating opinions from a large group of people can lead to accurate outcomes, as demonstrated in the 1907 Nature paper on guessing the weight of an ox.\"\n  },\n  {\n    \"question\": \"How can spammers affect the quality of data collected through crowdsourcing platforms like Amazon Mechanical Turk?\",\n    \"ideal_answer\": \"Spammers can produce low-quality annotations by optimizing for volume rather than accuracy. To mitigate their impact, different weighting schemes can be applied to downweight the contribution of spammers when measuring agreement between annotators.\"\n  },\n  {\n    \"question\": \"What is the purpose of using Cohen's Kappa in the context of rater agreement?\",\n    \"ideal_answer\": \"Cohen's Kappa is used to measure the inter-rater agreement by taking into account the agreement that could occur by chance. It provides a more accurate measure of agreement than raw percentages, especially when one label is more prevalent.\"\n  },\n  {\n    \"question\": \"In the context of data annotation, what does the descriptive paradigm advocate for?\",\n    \"ideal_answer\": \"The descriptive paradigm advocates for encouraging annotator subjectivity to model many beliefs. It emphasizes embracing diversity and identifying entries that are more subjective, recognizing that there can be more than one correct interpretation for some samples.\"\n  },\n  {\n    \"question\": \"How does the jury learning model differ from traditional models in handling annotator disagreement?\",\n    \"ideal_answer\": \"The jury learning model differs by modeling the different annotators' labeling behavior conditioned on their characteristics. It mimics the jury process, allowing for the specification of a group of jurors to determine a sampling strategy and aggregating labels from multiple trials to make a final decision.\"\n  },\n  {\n    \"question\": \"What are influence functions and how are they used in model training?\",\n    \"ideal_answer\": \"Influence functions are a technique from robust statistics to measure the effect of training data points by describing how model parameters change when a training point is upweighted by an infinitesimal amount. They can approximate leave-one-out retraining effects without actually running all the retraining, helping to identify mislabeled data.\"\n  },\n  {\n    \"question\": \"What does the Data Maps method track during training to analyze dataset quality?\",\n    \"ideal_answer\": \"The Data Maps method tracks the model's confidence in the true label and the variability of this confidence during training. These attributes help to analyze the quality of the dataset by identifying hard-to-learn samples that are more likely to be mislabeled.\"\n  },\n  {\n    \"question\": \"How does the AUM method help identify mislabeled data in a dataset?\",\n    \"ideal_answer\": \"The AUM (Area under the Margin) method identifies mislabeled data by tracking the margin between the assigned logit and the next largest logit over training epochs. Mislabeled samples tend to have smaller margins due to the tension between incorrect supervised signals and generalization driven by correct samples.\"\n  }\n]\n```"]